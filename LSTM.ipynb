{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91bbe3e",
   "metadata": {},
   "source": [
    "### Êï∞ÊçÆÂ§ÑÁêÜ"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 125,
=======
   "execution_count": 28,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 28,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "965c54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('amazon.csv')\n",
    "df = df[:300]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 165,
=======
   "execution_count": 29,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 29,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "d900057e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 165,
=======
     "execution_count": 29,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
     "execution_count": 29,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Words'] = df['review_body'].apply(lambda x: x.split(\" \"))\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "df['Words'] = df['Words'].str.lower()\n",
    "df['Words'] = df['Words'].astype(str)\n",
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
    "df['Count'] = df['review_body'].apply(lambda x: len(x.split(\" \")))\n",
    "rows_to_delete = df[df['Count'] < 10].index\n",
    "df.drop(rows_to_delete, inplace=True)\n",
    "\n",
    "max_seq_len = df.Count.max()\n",
    "max_seq_len "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 166,
=======
   "execution_count": 30,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 30,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "2ec8176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dictionary():\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<PAD>\" : 0}\n",
    "        self.idx2word = [\"<PAD>\"]\n",
    "    \n",
    "    def add(self,word):\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        word_lower = word.lower()\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            \n",
=======
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
    "        \n",
    "        word = word.lower()\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
<<<<<<< HEAD
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 167,
=======
   "execution_count": 31,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 31,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "6effcb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dictionary()\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "    for word in row.Words:\n",
    "        dic.add(word)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
   "id": "090125a4",
   "metadata": {},
   "outputs": [],
   "source": []
=======
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "execution_count": 53,
   "id": "090125a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', 'lots', 'of', 'ads<br', '/>slow', 'processing', 'speed<br', '/>occasionally', 'shuts', 'down', 'apps<br', '/>wifi', 'keeps', 'having', 'authentication', 'issues<br', '/><br', '/>was', 'cheap', 'for', 'a', 'tablet', 'and', 'now', 'i', 'know', 'why.', 'excellent', 'unit.', '', 'the', 'versatility', 'this', 'tablet,', 'besides', 'being', 'competitively', 'priced', 'is', 'solution', 'to', 'elderly.', 'poor', 'eyesight', 'physical', 'disabilities', 'associated', 'with', 'age', 'using', 'supporting', 'add', 'on', 'features', 'allows', 'user', 'stay', 'in', 'touch', 'our', 'changing', 'world.<br', '/>a', 'realistic', 'keyboard', 'that', 'you', 'can', 'see', 'use.<br', '/>i', 'cannot', 'wait', 'use', 'my', 'fire', 'hd7', 'show', 'computer', 'created', 'work', 'sheets', 'class', 'instructions', 'students.', '(hmdi)', 'good', 'by', 'copiers', 'reams', 'paper,', 'farwell', 'those', 'costly', 'printer', 'inks!<br', '/>oh', 'yes', 'have', 'much', 'more', 'gain', 'back.', 'just', 'takes', 'little', 'effort', 'learn', 'more,', 'open', 'book', 'read.<br', '/>noel', 'bought', 'amazon', 'prime', 'so', 'ended', 'up', 'buying', '16gb', 'one', '$95.', 'camera', 'okay', 'love', 'edit', 'do', 'pictures.', 'amazon/netflix/huliu', 'all', 'run', 'smoothly', 'tablet.', 'internet', 'runs', 'lot', 'faster', 'than', 'other', 'too.', 'while', 'over', 'am', 'happy', 'it', 'did', 'come', 'couple', 'disappointments', 'wanted', 'make', 'sure', 'people', 'are', 'aware', 'of.', 'an', 'member', 'thought', 'could', 'read', 'books', 'free', 'kindle', 'was', 'disappointed', 'only', 'borrow', 'month', 'free.', ':(', 'also', 'main', 'reasons', 'we', 'purchased', 'because', 'kids', 'plants', 'vs', 'zombies', 'games.', 'were', 'quite', '2', 'not', 'available', 'fire.', 'there', 'games/apps', 'looking', 'forward', 'previous', 'cheaper', 'neutab', 'recommend', 'ask', 'about', 'apps', 'they', 'before', 'buying.', 'works', 'well', 'but', 'battery', 'goes', 'dead', 'quickly', \"you'd\", 'expect.', 'really', 'version', 'weighs', 'less.', 'browser', 'considerably', 'better', 'still', 'has', 'issues', 'loading', 'certain', 'websites.', 'pop-up', 'ads', 'challenge', 'at', 'times.', 'same', 'as', 'older', 'version,', 'product', 'serves', 'its', 'purpose', \"wouldn't\", 'rave', 'it.', 'enjoy', 'new', 'kindle,', 'easy', 'offers', 'plenty', 'useful', 'entertaining', 'activities', \"it's\", 'what', 'performs', 'perfectly.', 'everything', 'great,', 'menu', 'system', 'awful.', \"i'd\", 'go', '5', 'stars', 'if', 'better.', 'old', 'kindles', 'does', 'more.', 'complaint', '&#34;grainy&#34;', 'after', 'trying', 'fix', 'problem', 'pictures', 'good.', 'got', 'promotional', 'price', 'very', 'worried', 'last', 'android', 'device', 'real', 'turkey.', 'reader', '.', 'downloaded', 'fairly', 'fast', 'no', 'problems,', 'from', 'overdrive', 'app.', 'need', 'traveling.', 'perfect', 'size', 'screen', 'resolution', 'great.', 'sound', 'quality', 'yet,', 'expect', 'be', 'most', 'apple', 'phones.', 'bends', 'backwards', 'keep', 'customers', 'happy.', 'appreciate', 'service.', 'like', 'hd', 'display.', 'used', 'live.', 'expected', 'already', 'gotten', 'hours', 'entertainment', 'information.', 'easier', 'from,', 'paperbacks', 'especially', 'vacation.', 'nice', 'deliberately', 'made', 'chromecast', 'impossible', \"don't\", 'waste', 'your', 'money', '7&#34;.', \"you'll\", '100', 'any', 'market.', 'limited', 'purposely', \"doesn't\", 'allow', 'downloads', \"aren't\", 'market', 'place.', 'soooo', 'many', 'problems', '!!', ':', 'slow', 'connection', 'internet,', 'possible', 'time,', 'or', 'videos,', 'games,', 'etc.', 'support!!!', 'reading', 'owners.', 'so,', 'yourself', 'huge', 'favor', 'buy', 'anything', 'else!!!', 'mine', 'gift', 'well-meaning', 'son.', 'glitchy', 'first', 'edition', 'flash', 'compatible', 'which', 'renders', 'useless.', 'freezes', 'constantly,', 'crap', 'life', 'regularly', 'occuring', 'page', 'last.', 'incredibly', 'disappointing.', 'it,', 'wish', 'download', 'without', 'service', 'kindle.', 'space', 'has.', 'device.', 'top', 'notch.', 'would', 'definately', \"i'm\", 'however,', 'second', 'year,', 'since', 'refuses', 'charge', 'up.', '(not', 'that!)', 'professional', 'set', 'me', 'hd.', 'so.', 'able', 'program', 'easily', 'own.', 'there.', '(in', 'clouds).', 'few', 'songs', 'enjoying', 'finally', 'gave', 'replaced', 'paperwhite', 'loved', 'think', 'going', 'well.', 'every', 'day', 'get', 'upset', 'when', 'accidentally', 'leave', 'home', 'me.', 'best', 'purchases', \"i've\", 'myself.', 'great', 'capabilities.', 'excited', 'choice', 'purchase', 'these.', \"won't\", 'connected', 'wifi,', 'constantly', 'log', 'matter', 'where', '(public,', 'home,', 'etc).', 'unit', 'came', 'directions', 'kind.', 'how', 'guess', 'will', 'overall', 'kids.', 'take', 'advantage', 'things', 'do.', 'some', 'confusing', 'loaded', 'then', 'given', 'profiles.', 'each', 'app', 'uses', 'must', 'profile.', 'taking', '3', 'places', 'delete', 'profile', 'others.', 'seem', 'removed', 'all!', 'shows', \"settings-it's\", 'confusing.', 'well,', 'fast!', 'yet', 'until', 'case', 'comes', 'fear', 'scratching', 'fab', 'display!', 'value', 'operating', 'buggy.', 'periodically', 'crashes', 'requires', 'rebooting.', 'find', 'regular', '(because', 'hold', 'side).', 'out', 'brighten', 'darken', 'text', 'reading.', 'lastly,', 'advertisements', 'time', 'turn', 'annoying.', 'returned.', 'prefer', 'far', 'quality...it', 'never', 'disappoint.', 'strict', 'amazonian', '-', 'you.<br', '/>the', 'personal', 'choice,', 'quality.', 'it!', 'convenient', 'library', 'books!', 'should', 'done', 'long', 'ago!', 'generation', 'perfected', 'placement', 'ports.', 'operation.', 'sound.', 'hd!!', 'appreciated.', 'amazing.', 'video', 'best.', 'bother', 'watching', 'news', 'television', 'anymore,', 'straight', 'success.', 'once', 'prontotec', '7&#34;', 'useless,', 'put', 'off', 'tablets', 'altogether.', 'actually', 'works,', 'quick,', 'responds', 'accurately', 'touch,', 'makes', 'ebook', 'reader.', 'been', 'crazy', 'fine', 'simple', 'checking', 'email', 'getting', 'notices', 'gmail', 'calendar.', 'uses,', 'involves', 'input,', 'rather', 'desktop', 'big', 'keyboard.', \"that's\", 'knock', 'fire,', 'preference.', 'point', \"haven't\", 'downloading', 'them.', 'sitting', 'chair', 'reading,', \"can't\", 'pc.<br', 'sells', 'below', 'cost', 'hope', 'buyers', 'spend', 'books,', 'movies,', 'subscription', 'services,', 'true,', 'really,', \"isn't\", 'content,', 'review', 'address', 'that.', 'enjoyed', 'trials', 'both', 'unlimited', '(ku).', 'paid', 'services', 'give', 'privileges', '&#34;free&#34;', 'stuff', '(that', 'paying', 'subscription).', 'decided', 'cancel', 'end', 'trial', 'period', 'next', 'week.', 'saying', 'worth', 'found', 'tv', 'series', '(bosch,', 'sneaky', 'pete)', 'ku.', 'list', 'against', 'ku', 'offerings,', 'them', 'money,', 'catalog.', 'pay', 'similarly,', 'movies', 'included', 'prime.', 'despite', 'these', 'streams', 'through', 'subscription,', 'full', 'retail', 'subscriptionif', 'holds', 'true.', 'ebooks', 'local', 'library.', \"i'll\", 'try', 'awhile.', 'check', 'always', 'offered', 'bad', 'are.', 'may', 'back', 'compared', 'experience.', 'start', 'subscriptions', 'services.', 'reading.<br', '/>your', 'needs', 'tastes', 'vary.', 'urge', 'consider', \"you're\", 'access', 'google', 'store', 'downloads.', \"amazon's\", 'store.', 'free,', 'necessarily', 'ones', 'phones', 'tablets.', 'also,', 'model', 'wi-fi', 'only,', 'cell', 'data', 'plans.', 'house,', 'want', 'spring', 'something', 'expensive', 'connected.', 'lite', 'carry', 'n', 'pocketbook', 'u', 'everywhere.', 'site', 'clear', 'safe.', 'device,', 'laptop.', 'explore', 'else', 'offer.', 'except', 'lasts', 'did.', 'five', 'stars,', 'request', 'exit', 'screen.', 'üòâ', 'use.', 'everywhere', 'changes', 'speed', 'two.', 'us.', 'them!!', '3rd', '2nd', 'liked', 'enough', 'storage', 'capacity', 'need.', 'games', 'internet.', 'product!', 'third', 'all.', 'support', 'sucks', 'rocks.', 'two', 'fires.', '7', '6.', 'connect', 'wifi', 'minutes.', 'tried', 'nothing', 'works.', 'another', 'fire...', 'offer', 'help', 'problem.', 'one.the', 'speakers', 'stink.sound', 'distorted', 'fuzzy', 'best.it', 'sounds', 'blown', \"it.it's\", 'fourth', 'had', 'again', \"wouldn't.\", 'small', 'e-mail', 'looks', 'fill', 'bill.', 'travel', 'laptop', 'home.', 'usb', 'port.', 'lve', 'different', 'hold.', 'durable!', 'cumbersome', 'pop', 'time.', 'carousel', 'hinders', 'helps.', 'hard', 'copy', 'instruction', 'manuals', 'way', 'print', 'ordered', 'one.<br', 'ok', 'money.', 'received', 'week', 'learning', 'things.', 'far,', 'thoroughly', 'avid', 'book.', 'times', 'wechat', 'installed', 'help!', 'birthday!!!', 'niece', 'nephew.', 'membership', 'songs!', 'picture', 'dramatically', 'improved', 'kudos', 'upgrade!!!', 'granddaughters', 'replacement', 'room.plus', 'black', 'care', 'white.', 'mother', 'started', '8', 'months', 'she', 'quit', 'working', 'days', '1', 'year', 'mark', 'refuse', 'replace', '(told', 'us', 'replacement).', 'said', 'even', 'their', 'products...why', 'sell', 'repair.', 'mom', 'fixed', 'income', 'afford', 'one.', '4', 'products', 'moment', 'rid', 'ipads', '(at', 'least', 'products)', 'product.', 'earlier', 'expected.', 'greatly', 'kindle!', 'freezes,with', 'paste.so', 'learn.turned', 'right', 'around', 'let', 'daughter', 'to!', '157.00', 'shipping', 'giving', 'item', 'week...the', 'stop', 'freezing', 'black...', 'refurbished', 'soon', 'unite', 'number', 'apps.', 'choices', 'devices.', 'retailer', 'settings', 'memory', 'instead', 'website', 'birthday', 'wonderful', 'surprise.', 'exceeded', 'expectations', 'performance', 'wise', 'everyone,', 'have.', 'crisp', 'picture,', '&', 'sleek', 'design..', 'pad', 'mini', 'gaming..fantastic', 'well!', 'star', 'plus.', 'friendly.', 'world', 'likes', 'online', 'everything,', 'owners', 'manual', 'mind', 'extra', 'although', 'like.', 'price.', 'variety', 'features.', 'entire', 'family.', 'convenience', 'ipad.', 'bit', 'issue.', 'three', 'years', 'sadly', 'lay', 'rest,', 'fault', 'entirely', 'mine.', 'ever', 'charger,', 'unless', 'plugged', 'specific', 'angle.', 'called', 'tech', 'sent', 'charge,', 'charger', 'woman', 'spoke', 'helpful', 'turned', 'worked', 'original', 'defective,', 'son', 'playing', 'occasionally', 'videos', 'downloaded.', 'daily', 'basis', 'sad', 'broke', 'taken', 'eye', 'doctor', 'appointment', 'waited', 'accident', 'happened.', 'rolled', 'glaucoma', 'testing', 'machine,', 'flew', 'lap', 'slammed', 'into', 'metal', 'base', 'machine...i', 'rolling', 'too', 'enthusiastically.', 'appeared', 'fine,', \"didn't\", 'scratch', 'half', 'blue/half', 'excuse', 'new,', 'larger', 'thinking', 'anyway.', 'understand', 'negative', 'reviews', 'all;', 'slam', 'machine', 'break', 'screen,', 'believing', 'falling', 'feet', 'grass', 'arm', 'living', 'room', 'suspect', 'exaggerated', 'own', \"kid's\", 'clumsiness.', 'highly', 'uses!', 'kids.<br', '/>as', 'parent,', 'options', 'provides', '(amazon', 'freetime)', 'parental', 'controls.<br', 'feel', 'secure', 'stumble', '4th', 'past', 'gifts.', 'sister', 'loves', 'reasonable', 'amazed', 'inch', 'book-size', 'look', 'magazines', 'color!', 'thank', 'developing', 'such', 'gadget.', 'techie', 'gal,', 'pretty', 'info', 'unable', 'figure', 'savvy.', 'wife,', 'spends', 'complaints.', 'special', 'sales', '$79,', 'cart,', 'contacted', 'explained', 'situation', 'sold', 'outfit.', 'concerns', 'volumn', 'faint', 'whisper', 'short', 'watch', 'length', 'continues.', 'return', 'necessary', 'label', 'printer.', 'touch.', 'ago.', 'faster!', 'advice:', 'gb.', 'decide', '7!', 'luckily,', 'single', 'issue', 'nook', 'problems.', 'hated', 'part', 'figured', 'anytime', 'want.', 'love!', 'become', 'fav', 'now,', 'amazing', 'browsing,', 'movie.', 'drawback', 'primary', 'agenda', 'anyways.', 'low', 'sound!!!', 'great!!!', 'alot', 'would!!', 'value!!!', 'expected...startup', 'location', 'control', 'buttons.', 'volume', 'on/off.', 'permit', 'hdmi', 'cable', 'netflix', 'otherwise', 'disappointing,', 'android?', 'play', 'available,', 'functional', 'instagram.', 'hoped,', 'overdrive/library', 'epub.', 'inexpensive', 'day,', 'met', 'though.', 'awesome!', 'starts', 'grab', 'bought.', 'myself', 'others', 'instructed', 'solved.', 'month,', 's', 'broken.', 'suddenly', 'longer', 'on.', 'power', 'button', '30s', 'reset', 'work.', 'charged', 'reliable.', 'waiting', 'hear', 'not.', 'bummed.', 'bother.', 'episode', '&#34;is', 'compatible&#34;', 'damned', 'e-reader,', 'this.', 'supplement', 'ipad', 'husband', 'july', '&#34;sale&#34;,', 'why', 'pick', 'gradually', 'laggy.', 'afraid', 'comparison.', 'loud,', '(volume,', 'on/off,', 'etc)', 'price,', 'ok,', \"they've\", 'gone', 'quality,', 'frequent', '&#34;reboot&#34;.', 'compare', 'tthis', 'encouraged', 'knows', 'ahve', 'turning', 'pages', 'book,', 'mom.', 'wit', 'space.', 'gb', 'yr', '(for', 'whom', 'bought)', 'took', 'sale', 'order', 'this!', 'fun', 'colors', 'brilliant.', \"couldn't\", 'happier', 'say?', 'rival', '0', 'white', 'color', 'disappointing', 'ever.<br', 'child,', '&#34;teen&#34;', 'teen', 'slightly,', 'all,', 'toddler', 'child', 'him/her', 'adult,', 'thereby', 'obtaining', 'content.<br', '/>amazon', 'ashamed', 'themselves', 'touting', 'terrific', '&#34;kid', 'friendly&#34;', 'freetime', 'controls', 'obviously', 'broken.<br', '/>parents', 'children', '9', 'elsewhere!!!', 'cams..very', 'basic', 'cam..returned', 'within', 'exactly', 'wanted!', 'music,', 'videos.', 'retired', 'kindal,', '!', 'it..', 'guide', 'making', 'shopping', 'icon', 'annoying.<br', '/>cant', 'locator', 'tracks', 'everywhere,', 'cant', 'off.<br', '/>video', 'far.', 'is.', 'sub', 'charging', 'cord.', 'longer.', 'thing', 'charging.', 'somewhere', 'ran', 'os.', 'during', 'sale.', 'customized', 'os,', 'call', 'cool', 'you.', 'basically', 'glad', 'ever!!', 'lasted', '4+', 'years,', 'cracked', 'granddaughter', 'proud', 'owner', 'second.', 'hand,', 'tethered', '(and', 'sometimes', 'uncomfortably)', 'chapter', 'phone', 'emergency,', 'heavy', 'wake', 'hits', 'face', 'fallen', 'asleep', ';)', 'newest', 'gets', 'designers', 'actual', 'users', 'putting', 'bottom', 'cycle.', 'port', 'nice.', 'otherwise,', 'job', 'amazon!', 'superb', 'understatement.', 'thanks', 'million.', 'late', 'christmas', 'present,', 'thing,', 'hoever,', 'nitpicks,', 'listed', 'below,', 'less', 'surface,', 'probably', 'lightwieght', 'fireos,', 'quad', 'core', '1.4', 'ghz', 'who', 'arent', 'savvy:', 'bigger', 'better,', 'means', 'speed)', 'processor,', 'powered', 'snapdragon,', 'believe.', 'nitpicks.<br', '/>1.', 'idling', '(screen', 'off,', 'charging,', 'etc.)', '80%', 'ram', 'has,', 'leaving', '200mb', 'fart', 'with.', '(bigger', ')<br', '/>2.', 'youtube/', 'services:', 'this,', 'not,', 'but,', 'current', 'os', 'look.', 'however:', '.apks', '(android', 'apps)', 'willing', 'accept', '.apk', 'causes', 'internal', 'damage', 'fire.<br', '/>3.', 'lag:', 'remember', '200', 'mbs', 'left', 'with?', 'forgot', 'factor', 'usage,', 'mb,', 'downs,', 'terms', 'keyboard,', 'services.<br', '/>and', 'complaints', 'scale', 'dream', 'display', 'unit,', 'it?', 'his/her', 'exceptional.', 'flight', 'iowa', 'oregon.', 'bright,', 'sunlight.', 'toshiba', 'thats', 'sure.', 'louder', 'crisp.', 'everyday.', 'discovering', 'bells', 'whistles.', 'locks', 'asked', '10', 'doing', 'satisfied', 'fire!!', 'do!', 'anniversary', 'he', 'still.', 'efficient.', 'seems', 'arrived-', 'charged.', 'logged', 'email,', 'netflix,', 'hulu,', 'hbogo', 'minutes', 'moderately', 'person.', 'watched', 'problems-', 'weird', '&#34;wth', 'happening?&#34;', 'issues,', 'viewing', 'vacation', 'miss', 'emails.', 'aps.', 'added', 'payment', 'plan', 'guilty', 'wonder', 'monthly', 'payments', 'hulu', 'netflix?', 'idea', 'repeating', 'all-at-once', 'annual', 'ach', 'might', 'forget', 'hit', 'account', 'though', 'firestick', 'navigate', 'lovely.', 'happy.still', 'ready', 'ordering', 'again.', 'kind', 'default', 'interface', 'interface.', 'econd', 'kindel', 'damaged', 'believe', '2012', 'version.', 'noticed', 'newer', 'stuff,', 'profiles,', 'password.', 'moved', 'location.', '2.5', 'hrs', 'drained', 'steady', 'surfing', 'net.', 'graphics', 'clear.', 'table', 'meets', 'expectations.', 'greatest.', 'problem:', 'hands,', 'desk', 'standard', 'printer,', 'portable,', 'read.', 'wherever', 'go.', 'reason', 'recharged', 'often', 'purchasing', 'gb<br', '/>is', 'hoped', 'joy', 'reading!', 'excellant!', 'great!', 'capacity.', 'stick', 'guest', 'perks', 'user.', 'taste.', 'samsung', 'lighter', 'pricey.', 'everyone', 'family', 'better!', 'lover.', 'kindle....its', 'to.', 'constant', 'off!!!', 'shocking.', 'personalize', 'saver.', 'writing', 'now!', 'mainly', 'pleasantly', 'surprised', 'also.', 'friends', \"it'd\", 'electronic', 'versions', 'school', 'elizabeth.', 'receipt', \"wasn't\", 'it:', 'paper', 'amazingly', 'dumb', 'imitation.', 'days.', 'figure?', 'perfect.', 'processor', 'decent', 'music', 'bored.', 'point.', 'forever.', 'selection', 'phenomenal.', 'access.', 'awesome.', 'penny.', 'previously', 'loved,', 'however', 'outside.', 'anymore.', 'reads', 'weight.', 'format', 'moving', 'another.', '&#34;frozen&#34;', 'shut', 'restart', 'change', 'staying', 'sticking', 'aspects.', 'displays', 'advertisement', 'user-friendly', 'particularly', 'it.<br', '/>while', 'technological', 'person,', 'technology', 'simpler', 'stressful.', 'awful', 'missing', 'popular', 'important', 'customize', 'additional', 'photo', 'blue', 'black.', 'mother.', 'nursing', 'her', 'wife', 'definite', 'step', 'one,', 'along', 'buttom', '(great)', 'thin', 'haveing', 'openning', 'says', 'error.', 'warranty', 'nothing.', 'again!!!!!!!', 'barely', 'locked', 'oh', 'yeah,', 'talking', 'someone', 'speaks', 'english', 'improvements', 'tablet...', 'promotion', 'buy,', 'previously,', 'bargain', 'weeks,', 'sharp', 'explored', 'busy', 'investment.', 'size.', 'along.', 'books.', 'stylus.', 'pleasure', 'trips', 'alone,', 'immensely.', 'unfortunately', '8gb', \"gb's\", 'amount', 'tv,', 'film', '7,7&#34;', 'wi-fi,', '16', 'first.', 'excellent.', 'returned', 'hubby', 'love,', 'cover', 'key', 'deal', '7(4th', 'generation).', 'exceptionally', 'vibrant', 'comments', '&#34;real&#34;', 'andriod,', 'load(or', 'side', 'load)', 'iphone.', 'managed', 'install', 'caused', 'play.', 'say', 'continue', 'function', 'normal.', 'apps,', \"customer's\", 'easier.', 'adequate', '98%', 'rare', 'occasion', 'sort', 'stop,', 'catch', 'up.<br', 'roughly', 'overwhelming', 'life.', 'daily.', 'replacing', 'completely', 'fair', 'comparison', 'utilities', 'visibility', 'possibly', 'some,', 'it?<br', '/>second', 'accessories.', 'confusion', 'various', '7,', 'generation.', 'finding', 'locally,', 'odd', 'amazon,', 'generic', 'cases', 'hooks', 'side.', 'amazon.com', 'supposed', 'work,', 'turns', \"wasn't.\", 'non-prime', 'seller', 'good,', 'case,', 'while.', 'unlike', 'starting', '&#34;i&#34;', 'galaxy,', 'accessories', 'live', 'town.<br', 'purchasedit', '&#34;prime', 'day&#34;', 'exceptional', 'value.', '$139?', 'galaxy', 'note', 'tab', 'primarily', 'sd', 'slot.', 'light', 'charging,and', 'lot.', 'that,', 'form', 'library,', 'touching', 'enjoyed.', 'adjust', 'light,', 'background', 'shade.', 'handy', 'sudoku,', 'web,', 'fingertips.', 'great<br', 'kindle-but', 'do,', 'loving', '6', '(which', 'love)', 'netflix!', 'interesting', 'upgrade', 'ads.', 'family<br', '/>!', 'pink', 'color.', 'itunes.', 'almost', 'andriod', 'glare.', \"'s\", 'ease', 'unparalleled.', 'camera.', 'photos', 'terrible.', 'needs,', 'instagram,', 'snap', 'chat', '(2013)', '2014', 'replete', 'nice,', 'memory.', 'fires,', 'liked.', 'process.', 'whether', 'day/night', 'symbol.', 'frustrated', '$79', 'purchase.', 'died.', 'couldnt', 'factory', 'replacement.', 'handled', 'e-reader', 'desiring', 'extensive', 'ebooks.', 'search', 'web', 'robust', 'barbie', 'dress', 'enjoys', 'likely', 'different.', 'shopping,', 'youtube,', 'aside', 'nicely', 'opinion.', 'chance,', 'mini.', 'lose', 'race', 'shot', 'process', 'learning.', 'plane', '(sob)', 'preordered', 'wait.', 'recently', 'cracked.', 'repaired.', 'under', 'warranty,', 'imagine', 'surprise', 'send', 'amazing!', 'anytime,', 'anywhere,', 'surf', 'movies.', 'best,', 'ever!', 'use,', 'size!', 'share', 'instant', \"child's\", 'chd', 'parent', 'inconvenient', 'thrilled', 'car', 'rides.<br', '/>update:<br', '/>', 'stars:', 'update', 'ago', 'whenever', 'automatically', 'boots', 'load', 'whichever', 'down.', 'causing', 'frustration', 'his', 'assistance.<br', '/>also,', 'preschool', 'in-app', 'purchases.', 'refund', 'response', \"parent's\", 'felt', 'defeated', 'controls.', \"we're\", 'mode', 'user-friendly--it', 'mode,', 'reboots', 'mode.', 'disappointed.', '(it', 'recognize', 'sometimes),', 'freeze', 'while,', 'apparent', 'email.', 'intrusive', 'contributed', 'pages.', 'once,', 'knew', 'fail.', 'graphics,', 'overall.', 'surfing.', 'taking,', 'large', 'ipad,', 'alternative.', 'weeks', 'now.', 'definitely', 'slower', 'done.', 'though,', 'disconnecting', \"(i'm\", 'experiencing', 'devices,', 'network).', '90', 'bucks', 'cyber', 'monday', 'etc...', 'crazed', 'young', 'fits', 'needs!', 'average', 'charges', 'fast.', '&#34;sponsored', 'ads&#34;', 'biggie.', 'needed', 'version.<br', '/>you', 'mobile', 'is,', 'fit', 'needs?', 'smartphone,', 'line', 'becomes', 'redundant', 'unnecessary.', 'fact', 'guarantee', 'pleased!', 'till', 'x-mas', 'luv', 'gives', 'life!!üòä', 'weight', 'stalling', 'restart.', 'hole', 'slow.', 'won', 'raffle.', 'pad.', 'beat.', 'response.', 'good!', 'terrible', 'information', 'o', 'help.', 'worst', 'owned!', 'fortunately,', 'warranty.', 'replacements', 'customer', 'replacement....3', 'final', 'blank.', 'itself', 'horrible', 'heard', 'before.', 'experience,', 'frustrating', 'froze,', 'switch', 'read,', 'docs', 'syncs', 'calendar', \"(i'd\", 'busier', 'proven', 'true!', 'time.)', 'treadmill', 'gym', 'purse.', 'followed', 'monthly.', 'could.', 'kindles,', 'right.', 'blank', 'skips', 'pages,', 'bring', 'sending', ',', ',i', 'friendly,', 'bang', 'buck.', 'im', 'gift.', 'person', 'enjoy.', 'fine.', 'in<br', 'complaining', 'pixel', 'sizes/', 'resolution.', 'tell', 'difference.', 'display...', 'gift!', 'here', 'use!', 'gen', 'improvement', 'stars.<br', '/>my', 'major', 'dim', 'enough.', 'range', 'dimming.', 'tends', 'conflict', 'allowed', 'dark', 'headache.<br', \"/>it's\", 'loved.', '4g,', 'range.', 'ordered,', 'minus.', 'desing.', 'online,', '(you', 'complicated', 'workarounds,', 'frustrating)', 'overall,', 'impressed,', 'family,', 'rest!']\n"
     ]
    }
   ],
   "source": [
    "print(dic.idx2word)"
   ]
<<<<<<< HEAD
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
  },
  {
   "cell_type": "markdown",
   "id": "7df5caf0",
   "metadata": {},
   "source": [
    "### Êï∞ÊçÆÈõÜÁöÑÊûÑÂª∫"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 168,
=======
   "execution_count": 39,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 39,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "0f788d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.utils.data import  Dataset, DataLoader\n",
    "class sentimentdataset(Dataset):\n",
    "    def __init__(self,df,max_seq_len):\n",
    "            self.data = df.Words.tolist()\n",
    "            self.label = df.star_rating.tolist()\n",
    "            self.max_seq_len= max_seq_len\n",
    "    def __getitem__(self,idx):\n",
    "        tokens = []\n",
    "        for word in self.data[idx]:\n",
    "            tokens.append(dic.word2idx[word])\n",
    "            \n",
    "        for  i  in  range(self.max_seq_len-len(tokens)):\n",
    "            tokens.append(dic.word2idx['<PAD>'])\n",
    "            \n",
    "        return torch.tensor(tokens).long(),torch.tensor(self.label[idx]).long()\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 169,
=======
   "execution_count": 40,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 40,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "43f4a57b",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "batch_sizes = 16\n",
=======
    "batch_sizes = 4\n",
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
    "batch_sizes = 4\n",
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
    "\n",
    "dataset = sentimentdataset(df,max_seq_len)\n",
    "dataloader = DataLoader(dataset,batch_sizes,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db57bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90787c65",
   "metadata": {},
   "source": [
    "### LSTMÊ®°ÂûãÊûÑÂª∫\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 170,
=======
   "execution_count": 41,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 41,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "638aa9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "import torch.nn.functional as F\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, max_seq_len, num_layers, hidden_dim, embedding_dim, vocab_sizes, dropout_rate=0.5):\n",
=======
    "\n",
    "class Languagemodel(nn.Module):\n",
    "    def __init__(self,max_seq_len,num_layers,hidden_dim,embedding_dim,vocab_sizes,dropout_rate = 0.5):\n",
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
    "\n",
    "class Languagemodel(nn.Module):\n",
    "    def __init__(self,max_seq_len,num_layers,hidden_dim,embedding_dim,vocab_sizes,dropout_rate = 0.5):\n",
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        self.embedding = nn.Embedding(vocab_sizes, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=dropout_rate,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(hidden_dim * max_seq_len, 5)  # Output dimension is 5 for multi-class classification\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        embedding = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(embedding, (hidden, cell))\n",
    "        y = self.dropout(output)\n",
    "        y = self.flatten(y)\n",
    "        y = torch.softmax(y, dim=1)  # Use softmax instead of sigmoid for multi-class classification\n",
    "        return y\n",
    "    \n",
    "    def init_hidden(self, batch_sizes):\n",
    "        hidden = torch.zeros(self.num_layers, batch_sizes, self.hidden_dim)\n",
    "        cell = torch.zeros(self.num_layers, batch_sizes, self.hidden_dim)\n",
    "        return hidden, cell\n"
=======
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
    "        self.embedding = nn.Embedding(vocab_sizes,embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,\n",
    "                           num_layers = num_layers,\n",
    "                           dropout = dropout_rate,\n",
    "                           batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim*max_seq_len,5)\n",
    "    \n",
    "    def forward(self,x,hidden,cell):\n",
    "        embedding = self.embedding(x)\n",
    "        output,hidden,cell = self.lstm(embedding,hidden,cell)\n",
    "        prediction = F.softmax(self.fc(self.drop(output)),dim=-1)\n",
    "        \n",
    "        return prediction,hidden,cell\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        return hidden, cell\n",
    "    \n",
    "   \n",
    "#      def init_weights(self):\n",
    "#         \"\"\"model intialize\"\"\"\n",
    "#         init_range_emb = 0.1\n",
    "#         init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "#         self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "#         self.fc.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "#         self.fc.bias.data.zero_()\n",
    "#         for i in range(self.num_layers):\n",
    "#             self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "#                 self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
    "#             self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim,\n",
    "#                     self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
<<<<<<< HEAD
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a6c2b",
   "metadata": {},
   "source": [
    "### ÂèÇÊï∞ËÆæÂÆö"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 171,
=======
   "execution_count": 42,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 42,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "b71692a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_layers = 2\n",
    "hidden_dim = 512\n",
    "embedding_dim = 256\n",
    "vocab_sizes = len(dic)\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "\n",
    "max_seq_len = 2362\n",
    "batch_sizes = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Languagemodel(max_seq_len,num_layers,hidden_dim,embedding_dim,vocab_sizes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d84c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
=======
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
    "max_seq_len = 2362\n",
    "batch_sizes = 4\n",
    "\n",
    "\n",
    "model = Languagemodel(max_seq_len,num_layers,hidden_dim,embedding_dim,vocab_sizes)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "loss = nn.CrossEntropyLoss()\n"
   ]
  },
  {
<<<<<<< HEAD
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "cell_type": "markdown",
   "id": "19f0dd08",
   "metadata": {},
   "source": [
    "### Ê®°ÂûãËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 172,
=======
   "execution_count": 47,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 47,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "6fe85dc0",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "def train_model():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        hidden, cell = model.init_hidden(batch_sizes)\n",
    "        hidden = hidden.to(device)\n",
    "        cell = cell.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x, hidden, cell)\n",
    "        \n",
    "        loss = F.cross_entropy(y_pred, y)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(\"Average loss:\", average_loss)\n",
    "    \n",
    "    model.eval()\n"
=======
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx,x,y in enumerate(dataloader):\n",
    "        hidden, cell = model.init_hidden(batch_sizes)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x,hidden,cell)\n",
    "        loss = (y,y_pred)\n",
    "        loss.backword()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        print(total_loss)\n",
    "            \n",
    "    model.eval()\n",
    "    "
<<<<<<< HEAD
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 173,
=======
   "execution_count": 49,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": 49,
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "id": "ff69783c",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
<<<<<<< HEAD
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 5, 512), got [2, 16, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-172-5c9b4c05d374>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-8fb34c73257e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden, cell)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    872\u001b[0m                 \u001b[1;31m# Each batch of the hidden state should match the input sequence that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# the user believes he/she is passing in.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m                 \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    788\u001b[0m                            ):\n\u001b[0;32m    789\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[0;32m    791\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[0;32m    792\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    257\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_weights_have_changed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 5, 512), got [2, 16, 512]"
=======
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
     "ename": "KeyError",
     "evalue": "'It'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-de0518ae88b3>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-d8de1092e0ba>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m  \u001b[0mi\u001b[0m  \u001b[1;32min\u001b[0m  \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'It'"
<<<<<<< HEAD
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 174,
   "id": "2ae02394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        hidden, cell = model.init_hidden(batch_sizes)\n",
    "        hidden = hidden.to(device)\n",
    "        cell = cell.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Test loss: {total_loss/len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481d7d9",
=======
   "execution_count": null,
   "id": "2ae02394",
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
=======
   "execution_count": null,
   "id": "2ae02394",
>>>>>>> c672d940be2a1ef75254ba103ad297dcc61913a0
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

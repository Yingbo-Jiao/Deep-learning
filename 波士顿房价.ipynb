{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bbe0cea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# 数据预处理\n",
    "def encode_categorical(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = label_encoder.fit_transform(df[column])\n",
    "        elif df[column].dtype == 'float64' or df[column].dtype == 'int64':\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "    return df\n",
    "\n",
    "df1 = pd.read_csv('train.csv')\n",
    "df1 = encode_categorical(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f262afc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "\n",
    "batch_sizes = 32\n",
    "\n",
    "x = df1.iloc[:,1:-1]\n",
    "y= df1.iloc[:,-1:]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2004)\n",
    "\n",
    "x_train_tensor = torch.FloatTensor(x_train.values)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)\n",
    "x_test_tensor = torch.FloatTensor(x_test.values)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values)\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = Data.TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = Data.DataLoader(train_dataset,batch_sizes,shuffle=True)\n",
    "test_dataloader = Data.DataLoader(test_dataset,batch_sizes,shuffle=True)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a45ed41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearNetWithSigmoid(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_features, 40)  # nn.Module\n",
    "        self.linear_2 = nn.Linear(40, 10)\n",
    "        self.linear_3 = nn.Linear(10, out_features)\n",
    "        self.sigmoid = nn.Sigmoid()  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.sigmoid(x)  \n",
    "        x = self.linear_2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.linear_3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e11e4312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4132],\n",
       "        [-0.4133],\n",
       "        [-0.1461],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1181],\n",
       "        [-0.3601],\n",
       "        [-0.3601],\n",
       "        [-0.1653],\n",
       "        [-0.1595],\n",
       "        [-0.2818],\n",
       "        [-0.3601],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1644],\n",
       "        [-0.1653],\n",
       "        [-0.2136],\n",
       "        [-0.1653],\n",
       "        [-0.3601],\n",
       "        [ 0.0649],\n",
       "        [-0.4133],\n",
       "        [-0.2047],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.2818],\n",
       "        [-0.3737],\n",
       "        [-0.1988],\n",
       "        [-0.3048],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1181],\n",
       "        [-0.4117],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.0356],\n",
       "        [-0.3737],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.3683],\n",
       "        [-0.1653],\n",
       "        [-0.3608],\n",
       "        [-0.4128],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4113],\n",
       "        [-0.4123],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4117],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.4117],\n",
       "        [-0.1653],\n",
       "        [-0.4117],\n",
       "        [-0.2385],\n",
       "        [-0.4486],\n",
       "        [-0.1665],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4132],\n",
       "        [-0.3601],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4460],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1664],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.0129],\n",
       "        [-0.1653],\n",
       "        [-0.4117],\n",
       "        [-0.1653],\n",
       "        [-0.4117],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.3601],\n",
       "        [-0.4133],\n",
       "        [-0.0902],\n",
       "        [-0.4133],\n",
       "        [-0.2936],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4110],\n",
       "        [-0.3737],\n",
       "        [-0.2820],\n",
       "        [-0.1731],\n",
       "        [-0.4133],\n",
       "        [-0.3755],\n",
       "        [-0.3737],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1219],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.4125],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1757],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4123],\n",
       "        [-0.1657],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4130],\n",
       "        [-0.0635],\n",
       "        [-0.4097],\n",
       "        [-0.1653],\n",
       "        [-0.4117],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4131],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.3601],\n",
       "        [-0.1653],\n",
       "        [-0.3750],\n",
       "        [-0.1282],\n",
       "        [-0.1363],\n",
       "        [-0.3601],\n",
       "        [-0.4133],\n",
       "        [-0.0356],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.3735],\n",
       "        [-0.1693],\n",
       "        [-0.4133],\n",
       "        [-0.4117],\n",
       "        [-0.0708],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.3737],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4117],\n",
       "        [-0.3592],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.3601],\n",
       "        [-0.1653],\n",
       "        [-0.1615],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [ 0.1296],\n",
       "        [-0.1653],\n",
       "        [-0.4097],\n",
       "        [-0.1143],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1676],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4103],\n",
       "        [-0.1624],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1517],\n",
       "        [-0.4106],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4131],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1722],\n",
       "        [-0.1653],\n",
       "        [-0.4112],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1517],\n",
       "        [-0.1629],\n",
       "        [ 0.1300],\n",
       "        [-0.1653],\n",
       "        [-0.4132],\n",
       "        [-0.1651],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.3601],\n",
       "        [-0.1652],\n",
       "        [-0.1532],\n",
       "        [-0.3737],\n",
       "        [-0.4133],\n",
       "        [-0.4117],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1651],\n",
       "        [-0.4118],\n",
       "        [-0.1181],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4117],\n",
       "        [-0.1651],\n",
       "        [-0.0784],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4117],\n",
       "        [-0.1940],\n",
       "        [-0.3601],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.4116],\n",
       "        [-0.4133],\n",
       "        [-0.1653],\n",
       "        [-0.4126],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4130],\n",
       "        [-0.4130],\n",
       "        [-0.3737],\n",
       "        [-0.3754],\n",
       "        [-0.4124],\n",
       "        [-0.4133],\n",
       "        [-0.4130],\n",
       "        [-0.1653],\n",
       "        [-0.1653],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.4133],\n",
       "        [-0.1653]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置优化器和损失函数\n",
    "import torch.optim as optim\n",
    "loss = nn.MSELoss()\n",
    "model = LinearNet(79,1)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "model(x_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bd714e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train_model(model,dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for idx,(x,y) in enumerate(dataloader):\n",
    "        y_pred = model(x)\n",
    "        cur_loss = loss(y_pred,y)\n",
    "        optimizer.zero_grad()\n",
    "        cur_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += cur_loss.item()\n",
    "    print(f\"train loss:{total_loss/len(train_dataset)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "56efe6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1206009564.9315069\n",
      "train loss:1209982579.7260275\n",
      "train loss:1207031920.2191782\n",
      "train loss:1213477670.5753424\n",
      "train loss:1208056374.3561645\n",
      "train loss:1208303517.8082192\n",
      "train loss:1203599828.1643836\n",
      "train loss:1212322810.739726\n",
      "train loss:1210287149.589041\n",
      "train loss:1204484455.4520547\n",
      "train loss:1202990095.7808218\n",
      "train loss:1207324470.3561645\n",
      "train loss:1203756670.2465754\n",
      "train loss:1213032837.260274\n",
      "train loss:1209658013.8082192\n",
      "train loss:1203720000.8767123\n",
      "train loss:1211018904.5479453\n",
      "train loss:1222467868.0547945\n",
      "train loss:1201804421.260274\n",
      "train loss:1208227443.7260275\n",
      "train loss:1205379010.630137\n",
      "train loss:1207464370.8493152\n",
      "train loss:1204507823.3424656\n",
      "train loss:1205865687.671233\n",
      "train loss:1210220640.4383562\n",
      "train loss:1217093777.5342467\n",
      "train loss:1214289734.1369863\n",
      "train loss:1205551708.9315069\n",
      "train loss:1213783602.8493152\n",
      "train loss:1207274227.7260275\n",
      "train loss:1205523712.0\n",
      "train loss:1206202562.630137\n",
      "train loss:1204655566.9041095\n",
      "train loss:1205153358.9041095\n",
      "train loss:1208140533.4794521\n",
      "train loss:1204457789.369863\n",
      "train loss:1208647508.1643836\n",
      "train loss:1206368778.5205479\n",
      "train loss:1210433574.5753424\n",
      "train loss:1204366583.2328768\n",
      "train loss:1204967166.2465754\n",
      "train loss:1222627328.0\n",
      "train loss:1206823560.7671232\n",
      "train loss:1202845169.9726028\n",
      "train loss:1204174840.9863014\n",
      "train loss:1202350146.630137\n",
      "train loss:1204186711.671233\n",
      "train loss:1211281783.2328768\n",
      "train loss:1206868609.7534246\n",
      "train loss:1205219152.6575344\n",
      "train loss:1204330993.9726028\n",
      "train loss:1206838252.7123287\n",
      "train loss:1211023829.9178083\n",
      "train loss:1209366933.041096\n",
      "train loss:1207039733.4794521\n",
      "train loss:1204313742.0273972\n",
      "train loss:1209528605.8082192\n",
      "train loss:1206451350.7945206\n",
      "train loss:1211165973.041096\n",
      "train loss:1206350271.1232877\n",
      "train loss:1214272685.589041\n",
      "train loss:1209391170.630137\n",
      "train loss:1204715211.3972602\n",
      "train loss:1208869642.5205479\n",
      "train loss:1207169013.4794521\n",
      "train loss:1217681507.9452055\n",
      "train loss:1209829537.3150685\n",
      "train loss:1212514268.9315069\n",
      "train loss:1210125952.0\n",
      "train loss:1207439640.5479453\n",
      "train loss:1224840961.7534246\n",
      "train loss:1207476471.2328768\n",
      "train loss:1209596535.2328768\n",
      "train loss:1209578669.589041\n",
      "train loss:1212761342.2465754\n",
      "train loss:1209852456.328767\n",
      "train loss:1207092274.8493152\n",
      "train loss:1207070383.3424656\n",
      "train loss:1208251774.2465754\n",
      "train loss:1207825606.1369863\n",
      "train loss:1207369414.1369863\n",
      "train loss:1208420371.2876713\n",
      "train loss:1205070995.2876713\n",
      "train loss:1216487168.0\n",
      "train loss:1204409154.630137\n",
      "train loss:1205789352.328767\n",
      "train loss:1208699851.3972602\n",
      "train loss:1207965659.1780822\n",
      "train loss:1204364300.2739725\n",
      "train loss:1208414409.6438355\n",
      "train loss:1206067466.5205479\n",
      "train loss:1205933099.8356164\n",
      "train loss:1204431095.2328768\n",
      "train loss:1207180559.7808218\n",
      "train loss:1208494395.6164384\n",
      "train loss:1208709889.7534246\n",
      "train loss:1209142699.8356164\n",
      "train loss:1217725126.1369863\n",
      "train loss:1212319838.6849315\n",
      "train loss:1206633272.109589\n"
     ]
    }
   ],
   "source": [
    "Epoch = 100\n",
    "for i in range(Epoch):\n",
    "    train_model(model,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4aff9460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证模型\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        y_pred = model(x)\n",
    "        cur_loss = loss(y_pred, y)\n",
    "        total_loss += cur_loss.item()\n",
    "    print(f\"Test loss: {total_loss/len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4d04abf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Epoch 0 ====\n",
      "train loss:1211683392.8767123\n",
      "Test loss: 1410238218.5205479\n",
      "==== Epoch 1 ====\n",
      "train loss:1208887553.7534246\n",
      "Test loss: 1424549439.1232877\n",
      "==== Epoch 2 ====\n",
      "train loss:1210246776.9863014\n",
      "Test loss: 1827541083.1780822\n",
      "==== Epoch 3 ====\n",
      "train loss:1204169854.2465754\n",
      "Test loss: 1409016242.8493152\n",
      "==== Epoch 4 ====\n",
      "train loss:1210650806.3561645\n",
      "Test loss: 1433728000.0\n",
      "==== Epoch 5 ====\n",
      "train loss:1207500782.4657533\n",
      "Test loss: 1844194710.7945206\n",
      "==== Epoch 6 ====\n",
      "train loss:1204756536.109589\n",
      "Test loss: 1398098424.9863014\n",
      "==== Epoch 7 ====\n",
      "train loss:1208391690.5205479\n",
      "Test loss: 1413409413.260274\n",
      "==== Epoch 8 ====\n",
      "train loss:1207075489.3150685\n",
      "Test loss: 1384307150.9041095\n",
      "==== Epoch 9 ====\n",
      "train loss:1209338017.3150685\n",
      "Test loss: 1488872938.958904\n",
      "==== Epoch 10 ====\n",
      "train loss:1207906167.2328768\n",
      "Test loss: 1422450337.3150685\n",
      "==== Epoch 11 ====\n",
      "train loss:1210043216.6575344\n",
      "Test loss: 1451432258.630137\n",
      "==== Epoch 12 ====\n",
      "train loss:1203827296.4383562\n",
      "Test loss: 1371583081.2054794\n",
      "==== Epoch 13 ====\n",
      "train loss:1204185400.109589\n",
      "Test loss: 1457730265.4246576\n",
      "==== Epoch 14 ====\n",
      "train loss:1206243343.7808218\n",
      "Test loss: 1570240736.4383562\n",
      "==== Epoch 15 ====\n",
      "train loss:1207626541.589041\n",
      "Test loss: 1458564530.8493152\n",
      "==== Epoch 16 ====\n",
      "train loss:1208849799.0136986\n",
      "Test loss: 1427262183.4520547\n",
      "==== Epoch 17 ====\n",
      "train loss:1206282282.0821917\n",
      "Test loss: 1421685570.630137\n",
      "==== Epoch 18 ====\n",
      "train loss:1208424076.2739725\n",
      "Test loss: 1431105465.8630137\n",
      "==== Epoch 19 ====\n",
      "train loss:1209885319.0136986\n",
      "Test loss: 1451235110.5753424\n",
      "==== Epoch 20 ====\n",
      "train loss:1206243996.0547945\n",
      "Test loss: 1393390269.369863\n",
      "==== Epoch 21 ====\n",
      "train loss:1204132225.7534246\n",
      "Test loss: 1476011449.8630137\n",
      "==== Epoch 22 ====\n",
      "train loss:1208785393.9726028\n",
      "Test loss: 1411455032.109589\n",
      "==== Epoch 23 ====\n",
      "train loss:1204790433.3150685\n",
      "Test loss: 1796842496.0\n",
      "==== Epoch 24 ====\n",
      "train loss:1206060100.3835616\n",
      "Test loss: 1434245709.1506848\n",
      "==== Epoch 25 ====\n",
      "train loss:1210708721.9726028\n",
      "Test loss: 1463905546.5205479\n",
      "==== Epoch 26 ====\n",
      "train loss:1210192326.1369863\n",
      "Test loss: 1538918168.5479453\n",
      "==== Epoch 27 ====\n",
      "train loss:1209115125.4794521\n",
      "Test loss: 1407350503.4520547\n",
      "==== Epoch 28 ====\n",
      "train loss:1211967228.4931507\n",
      "Test loss: 1438413550.4657533\n",
      "==== Epoch 29 ====\n",
      "train loss:1202144422.5753424\n",
      "Test loss: 1394142074.739726\n",
      "==== Epoch 30 ====\n",
      "train loss:1203750264.9863014\n",
      "Test loss: 1436928939.8356164\n",
      "==== Epoch 31 ====\n",
      "train loss:1212657276.4931507\n",
      "Test loss: 1369715487.5616438\n",
      "==== Epoch 32 ====\n",
      "train loss:1209076064.4383562\n",
      "Test loss: 1392414656.8767123\n",
      "==== Epoch 33 ====\n",
      "train loss:1208375325.8082192\n",
      "Test loss: 1413815850.0821917\n",
      "==== Epoch 34 ====\n",
      "train loss:1203969081.8630137\n",
      "Test loss: 1491395506.8493152\n",
      "==== Epoch 35 ====\n",
      "train loss:1206311583.5616438\n",
      "Test loss: 1444356208.2191782\n",
      "==== Epoch 36 ====\n",
      "train loss:1205349114.739726\n",
      "Test loss: 1443668746.5205479\n",
      "==== Epoch 37 ====\n",
      "train loss:1206494364.0547945\n",
      "Test loss: 1436750876.0547945\n",
      "==== Epoch 38 ====\n",
      "train loss:1208659876.8219178\n",
      "Test loss: 1622543640.5479453\n",
      "==== Epoch 39 ====\n",
      "train loss:1208219709.369863\n",
      "Test loss: 1457811820.7123287\n",
      "==== Epoch 40 ====\n",
      "train loss:1206191249.5342467\n",
      "Test loss: 1429882844.9315069\n",
      "==== Epoch 41 ====\n",
      "train loss:1208022580.6027398\n",
      "Test loss: 1409439365.260274\n",
      "==== Epoch 42 ====\n",
      "train loss:1203262236.0547945\n",
      "Test loss: 1435536797.8082192\n",
      "==== Epoch 43 ====\n",
      "train loss:1212701236.6027398\n",
      "Test loss: 1858517405.8082192\n",
      "==== Epoch 44 ====\n",
      "train loss:1210225914.739726\n",
      "Test loss: 1546628110.0273972\n",
      "==== Epoch 45 ====\n",
      "train loss:1207353379.0684931\n",
      "Test loss: 1385709603.0684931\n",
      "==== Epoch 46 ====\n",
      "train loss:1205279077.69863\n",
      "Test loss: 1570252147.7260275\n",
      "==== Epoch 47 ====\n",
      "train loss:1206026234.739726\n",
      "Test loss: 1415049615.7808218\n",
      "==== Epoch 48 ====\n",
      "train loss:1211265381.69863\n",
      "Test loss: 1417754736.2191782\n",
      "==== Epoch 49 ====\n",
      "train loss:1209705191.4520547\n",
      "Test loss: 1516461673.2054794\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 50\n",
    "for i in range(num_epoch):\n",
    "    print(f\"==== Epoch {i} ====\")\n",
    "    train_model(model, train_dataloader)\n",
    "    test_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7fc4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a33a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
